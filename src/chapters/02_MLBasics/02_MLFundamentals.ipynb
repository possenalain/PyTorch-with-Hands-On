{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "## Gradients\n",
    "## Backpropagation\n",
    "## Update Rule\n",
    "## Gradient Descent\n",
    "## Optimizers\n",
    "## torch.nn\n",
    "## sequential and functional models\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradients**\n",
    "```markdown\n",
    "These are partial derivatives of a function with respect to its parameters. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([4.], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f578c729c90>\n"
     ]
    }
   ],
   "source": [
    "# The autograd package provides automatic differentiation \n",
    "# for all operations on Tensors\n",
    "# requires_grad = True -> tracks all operations on the tensor. \n",
    "x = torch.tensor([1], dtype=torch.float32, requires_grad=True)\n",
    "y = x*2 + 2 # dy/dx = 2\n",
    "\n",
    "# y was created as a result of an operation, so it has a grad_fn attribute.\n",
    "# grad_fn: references a Function that has created the Tensor\n",
    "print(x) # created by the user -> grad_fn is None\n",
    "print(y)\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([48.], grad_fn=<MulBackward0>)\n",
      "tensor(48., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Grad_fn will be tracked on all operations with y \n",
    "z = y * y * 3 # dz/dy = 6y -> dz/dx = dz/dy * dy/dx\n",
    "print(z)\n",
    "z = z.mean()\n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([48.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's compute the gradients with backpropagation\n",
    "# When we finish our computation we can call .backward() and have all the gradients computed automatically.\n",
    "# The gradient for this tensor will be accumulated into .grad attribute.\n",
    "# It is the partial derivate of the function w.r.t. the tensor\n",
    "\n",
    "z.backward()\n",
    "print(x.grad) # dz/dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Snippet of a typical training loop.**\n",
    "\n",
    "```markdown\n",
    "# Training loop\n",
    "- Forward Pass (W^{T}X + b)\n",
    "- Compute Loss\n",
    "- Compute Gradients (`loss.backward()`)\n",
    "- Update Weights (`optimizer.step()`)\n",
    "- Zero Gradients (`optimizer.zero_grad()`)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "# This is the parameter we want to optimize -> requires_grad=True\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "# b = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forward pass to compute loss\n",
    "y_predicted = w * x \n",
    "loss = (y_predicted - y)**2\n",
    "\n",
    "# backward pass to compute gradient dLoss/dw\n",
    "loss.backward()\n",
    "\n",
    "# update weights, this operation should not be part of the computational graph\n",
    "with torch.no_grad():\n",
    "    w -= 0.01 * w.grad\n",
    "    # b -= 0.01 * b.grad\n",
    "    ## optimizer.step() -> if optimizer is used\n",
    "# don't forget to zero the gradients\n",
    "w.grad.zero_()\n",
    "# b.grad.zero_()\n",
    "\n",
    "# Optimizer has zero_grad() method\n",
    "# optimizer = torch.optim.SGD([weights,biases], lr=0.01)\n",
    "# During training:\n",
    "# optimizer.zero_grad()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "```markdown\n",
    "Iteartively updates the weights of the model in order to minimize the loss function. \n",
    "```\n",
    "$w_{t+1} = w_{t} - lr * \\frac{dl}{dw}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 1.00-> y_pred: 1.00 -> loss: 1.00\n",
      "w: 1.20-> y_pred: 1.20 -> loss: 0.64\n",
      "w: 1.36-> y_pred: 1.36 -> loss: 0.41\n",
      "w: 1.49-> y_pred: 1.49 -> loss: 0.26\n",
      "w: 1.59-> y_pred: 1.59 -> loss: 0.17\n",
      "w: 1.67-> y_pred: 1.67 -> loss: 0.11\n",
      "w: 1.74-> y_pred: 1.74 -> loss: 0.07\n",
      "w: 1.79-> y_pred: 1.79 -> loss: 0.04\n",
      "w: 1.83-> y_pred: 1.83 -> loss: 0.03\n",
      "w: 1.87-> y_pred: 1.87 -> loss: 0.02\n"
     ]
    }
   ],
   "source": [
    "# INCREASE THE NUMBER OF EPOCHS\n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "lr = 0.1\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    y_predicted = w * x \n",
    "    loss = (y_predicted - y)**2\n",
    "    \n",
    "    print(f\"w: {w.item():.2f}-> y_pred: {y_predicted.item():.2f} -> loss: {loss.item():.2f}\")\n",
    "\n",
    "    # backward pass to compute gradient dLoss/dw\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad\n",
    "        \n",
    "    w.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTIMIZERS**\n",
    "```markdown\n",
    "`torch.optim` provides implementations of various optimization algorithms.\n",
    "\n",
    "- SGD\n",
    "- Adam\n",
    "- RMSprop\n",
    "- Adagrad\n",
    "\n",
    "optimizers are usefull in updating the weights of the model. \n",
    "\n",
    "optimizers = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 0.00 -> loss: 30.00\n",
      "w: 3.00 -> loss: 7.50\n",
      "w: 1.50 -> loss: 1.88\n",
      "w: 2.25 -> loss: 0.47\n",
      "w: 1.88 -> loss: 0.12\n",
      "w: 2.06 -> loss: 0.03\n",
      "w: 1.97 -> loss: 0.01\n",
      "w: 2.02 -> loss: 0.00\n",
      "w: 1.99 -> loss: 0.00\n",
      "w: 2.00 -> loss: 0.00\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Linear regression\n",
    "# f = w * x \n",
    "# end goal : f = 2 * x\n",
    "\n",
    "# dataset\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "LR = 0.1\n",
    "EPOCHS = 10\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD([w], lr=LR)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    #forward pass\n",
    "    y_predicted = forward(X)\n",
    "\n",
    "    # loss\n",
    "    l = torch.nn.MSELoss()(Y, y_predicted)\n",
    "    print(f\"w: {w.item():.2f} -> loss: {l.item():.2f}\")\n",
    "\n",
    "    # calculate gradients \n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**torch.nn.Module**\n",
    "```markdown\n",
    "- Base class for all neural network modules.\n",
    "- Your models should also subclass this class.\n",
    "- It has a `forward` method that defines the computation performed at every call.\n",
    "- Layers also are subclasses of `nn.Module`\n",
    "```\n",
    "\n",
    "```python\n",
    "class LinearModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # Define layers\n",
    "        self.linearLayer = torch.nn.Linear(input_dim, output_dim)\n",
    "        ...\n",
    "    def forward(self, x):\n",
    "        output = self.linearLayer(x)\n",
    "        return output\n",
    "model = LinearModel(input_size, output_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: torch.Size([4, 1]), target:torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Dataset\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32).unsqueeze(1)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "print(f\"input_shape: {X.shape}, target:{Y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of Sequential(\n",
       "  (L1): Linear(in_features=1, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 1 MODEL\n",
    "# model = nn.Linear(in_features=X.shape[1], out_features=Y.shape[1])\n",
    "\n",
    "# 2 Sequential API\n",
    "# model = nn.Sequential(\n",
    "#     nn.Linear(in_features=X.shape[1], out_features=Y.shape[1])\n",
    "# )\n",
    "\n",
    "# model = nn.Sequential(\n",
    "#     OrderedDict([(\"L1\",nn.Linear(in_features=X.shape[1], out_features=Y.shape[1]))])\n",
    "# )\n",
    "\n",
    "#3 Custom model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        # define layers\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output\n",
    "    \n",
    "model = LinearModel(input_dim=X.shape[1], output_dim=Y.shape[1])\n",
    "model.parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: 2.00 -> loss: 41.43\n",
      "w: 2.00 -> loss: 18.66\n",
      "w: 2.00 -> loss: 8.44\n",
      "w: 2.00 -> loss: 3.84\n",
      "w: 2.00 -> loss: 1.78\n",
      "w: 2.00 -> loss: 0.84\n",
      "w: 2.00 -> loss: 0.42\n",
      "w: 2.00 -> loss: 0.23\n",
      "w: 2.00 -> loss: 0.14\n",
      "w: 2.00 -> loss: 0.10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LR = 0.1\n",
    "EPOCHS = 10\n",
    "\n",
    "# loss \n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    #forward pass\n",
    "    y_predicted = model(X)\n",
    "\n",
    "    # loss\n",
    "    l = loss(Y, y_predicted)\n",
    "    print(f\"w: {w.item():.2f} -> loss: {l.item():.2f}\")\n",
    "\n",
    "    # calculate gradients \n",
    "    l.backward()\n",
    "\n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradients after updating\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
